{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "AC_CUDA_C-checkpoint.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vUO-_twSrxFh"
      },
      "source": [
        "<div align=\"center\"><h1>Accelerating Applications with CUDA C/C++</h1></div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_tVJ7A4P-aVG",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks/CUDA')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k5ngSqJ4rxFl",
        "outputId": "50f444fd-7b2b-4c08-ef69-12d2eb14c14c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat May 30 02:18:23 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tebpzSYlrxFr"
      },
      "source": [
        "---\n",
        "## Writing Application Code for the GPU\n",
        "\n",
        "CUDA provides extensions for many common programming languages, such as C/C++. This allows to easily run functions in their source code on a GPU.\n",
        "\n",
        "Below is a `.cu` (file extension for CUDA-accelerated programs).\n",
        "\n",
        "```cpp\n",
        "\n",
        "__global__ void GPUFunction()\n",
        "{\n",
        "  printf(\"This function is defined to run on the GPU.\\n\");\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "  CPUFunction();\n",
        "\n",
        "  GPUFunction<<<1, 1>>>();\n",
        "  cudaDeviceSynchronize();\n",
        "}\n",
        "```\n",
        "\n",
        "Here are some important lines of code to highlight, as well as some other common terms used in accelerated computing:\n",
        "\n",
        "`__global__ void GPUFunction()`\n",
        "  - The `__global__` keyword indicates that the following function will run on the GPU, and can be invoked **globally**, which in this context means either by the CPU, or, by the GPU.\n",
        "  - Often, code executed on the CPU is referred to as **host** code, and code running on the GPU is referred to as **device** code.\n",
        "  - Notice the return type `void`. It is required that functions defined with the `__global__` keyword return type `void`.\n",
        "\n",
        "`GPUFunction<<<1, 1>>>();`\n",
        "  - Typically, when calling a function to run on the GPU, we call this function a **kernel**, which is **launched**.\n",
        "  - When launching a kernel, we must provide an **execution configuration**, which is done by using the `<<< ... >>>` syntax just prior to passing the kernel any expected arguments.\n",
        "  - At a high level, execution configuration allows programmers to specify the **thread hierarchy** for a kernel launch, which defines the number of thread groupings (called **blocks**), as well as how many **threads** to execute in each block.\n",
        "\n",
        "`cudaDeviceSynchronize();`\n",
        "  - Unlike much C/C++ code, launching kernels is **asynchronous**: the CPU code will continue to execute *without waiting for the kernel launch to complete*.\n",
        "  - A call to `cudaDeviceSynchronize`, a function provided by the CUDA runtime, will cause the host (CPU) code to wait until the device (GPU) code completes, and only then resume execution on the CPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SU1Eh8v4rxFs",
        "outputId": "545bca6b-9d33-442d-8b1d-5e6dc17d1220",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!nvcc -o hello-gpu 01-hello-gpu.cu -run"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello from the CPU.\n",
            "Hello from the GPU.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nitCrIcDrxFv"
      },
      "source": [
        "---\n",
        "### Compiling and Running Accelerated CUDA Code\n",
        "\n",
        "`nvcc` will be very familiar to experienced `gcc` users. Compiling, for example, a `some-CUDA.cu` file, is simply:\n",
        "\n",
        "`nvcc -arch=sm_70 -o out some-CUDA.cu -run`\n",
        "  - `nvcc` is the command line command for using the `nvcc` compiler.\n",
        "  - `some-CUDA.cu` is passed as the file to compile.\n",
        "  - The `o` flag is used to specify the output file for the compiled program.\n",
        "  - The `arch` flag indicates for which **architecture** the files must be compiled. \n",
        "  - As a matter of convenience, providing the `run` flag will execute the successfully compiled binary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JCgz9rChrxFw"
      },
      "source": [
        "---\n",
        "## CUDA Thread Hierarchy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6nCy9hggrxFy"
      },
      "source": [
        "---\n",
        "## Launching Parallel Kernels\n",
        "\n",
        "The execution configuration allows programmers to specify details about launching the kernel to run in parallel on multiple GPU **threads**. More precisely, the execution configuration allows programmers to specifiy how many groups of threads - called **thread blocks**, or just **blocks** - and how many threads they would like each thread block to contain. The syntax for this is:\n",
        "\n",
        "`<<< NUMBER_OF_BLOCKS, NUMBER_OF_THREADS_PER_BLOCK>>>`\n",
        "\n",
        "** The kernel code is executed by every thread in every thread block configured when the kernel is launched**.\n",
        "\n",
        "Thus, under the assumption that a kernel called `someKernel` has been defined, the following are true:\n",
        "  - `someKernel<<<1, 10>>()` is configured to run in a single thread block which has 10 threads and will therefore run 10 times.\n",
        "  - `someKernel<<<10, 10>>()` is configured to run in 10 thread blocks which each have 10 threads and will therefore run 100 times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eLXfMj20rxFz",
        "outputId": "ed001f8b-ee60-4e8f-f0d7-15bebaa9ae21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "# Calling a function in parallel with 5 blocks and 5 threads per block (total 25)\n",
        "!nvcc -o basic-parallel 01-basic-parallel.cu -run"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is running in parallel.\n",
            "This is running in parallel.\n",
            "This is running in parallel.\n",
            "This is running in parallel.\n",
            "This is running in parallel.\n",
            "This is running in parallel.\n",
            "This is running in parallel.\n",
            "This is running in parallel.\n",
            "This is running in parallel.\n",
            "This is running in parallel.\n",
            "This is running in parallel.\n",
            "This is running in parallel.\n",
            "This is running in parallel.\n",
            "This is running in parallel.\n",
            "This is running in parallel.\n",
            "This is running in parallel.\n",
            "This is running in parallel.\n",
            "This is running in parallel.\n",
            "This is running in parallel.\n",
            "This is running in parallel.\n",
            "This is running in parallel.\n",
            "This is running in parallel.\n",
            "This is running in parallel.\n",
            "This is running in parallel.\n",
            "This is running in parallel.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8cl05sZUrxF3"
      },
      "source": [
        "---\n",
        "\n",
        "## CUDA-Provided Thread Hierarchy Variables\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "g4vSGVnqrxF5"
      },
      "source": [
        "---\n",
        "## Thread and Block Indices\n",
        "\n",
        "Each thread is given an index within its thread block, starting at `0`. Additionally, each block is given an index, starting at `0`. Just as threads are grouped into thread blocks, blocks are grouped into a **grid**, which is the highest entity in the CUDA thread hierarchy.\n",
        "\n",
        "CUDA kernels have access to special variables identifying both the index of the thread (within the block) that is executing the kernel, and, the index of the block (within the grid) that the thread is within. These variables are `threadIdx.x` and `blockIdx.x` respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "clbAvc7trxF6",
        "outputId": "3c8f40fe-83c2-4d82-dfb5-564559541c7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# print success when the 1023-th thread is executed (and block index == 255) \n",
        "!nvcc -o thread-and-block-idx 01-thread-and-block-idx.cu -run"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wHqHhGNirxF8"
      },
      "source": [
        "---\n",
        "## Accelerating For Loops\n",
        "\n",
        "For loops in CPU-only applications are ripe for acceleration: rather than run each iteration of the loop serially, each iteration of the loop can be run in parallel in its own thread. Consider the following for loop, and notice, though it is obvious, that it controls how many times the loop will execute, as well as defining what will happen for each iteration of the loop:\n",
        "\n",
        "```cpp\n",
        "int N = 2<<20;\n",
        "for (int i = 0; i < N; ++i)\n",
        "{\n",
        "  printf(\"%d\\n\", i);\n",
        "}\n",
        "```\n",
        "\n",
        "In order to parallelize this loop, 2 steps must be taken:\n",
        "\n",
        "- A kernel must be written to do the work of a **single iteration of the loop**.\n",
        "- Because the kernel will be agnostic of other running kernels, the execution configuration must be such that the kernel executes the correct number of times, for example, the number of times the loop would have iterated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W3TF1ouqrxF9",
        "outputId": "ec02d9b4-0835-4d33-a659-ee97f3b9269b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# accelerating a for loop on GPU by running on parallel (here printing the iteration of the for loop)\n",
        "!nvcc -o single-block-loop 01-single-block-loop.cu -run"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is iteration number 0\n",
            "This is iteration number 1\n",
            "This is iteration number 2\n",
            "This is iteration number 3\n",
            "This is iteration number 4\n",
            "This is iteration number 5\n",
            "This is iteration number 6\n",
            "This is iteration number 7\n",
            "This is iteration number 8\n",
            "This is iteration number 9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Spp1iiXLrxGA"
      },
      "source": [
        "---\n",
        "## Coordinating Parallel Threads\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HDfbfGlirxGD"
      },
      "source": [
        "---\n",
        "## Using Block Dimensions for More Parallelization\n",
        "\n",
        "There is a limit to the number of threads that can exist in a thread block: 1024 to be precise. In order to increase the amount of parallelism in accelerated applications, we must be able to coordinate among multiple thread blocks.\n",
        "\n",
        "CUDA Kernels have access to a special variable that gives the number of threads in a block: `blockDim.x`. Using this variable, in conjunction with `blockIdx.x` and `threadIdx.x`, increased parallelization can be accomplished by organizing parallel execution accross multiple blocks of multiple threads with the idiomatic expression `threadIdx.x + blockIdx.x * blockDim.x`. Here is a detailed example.\n",
        "\n",
        "The execution configuration `<<<10, 10>>>` would launch a grid with a total of 100 threads, contained in 10 blocks of 10 threads. We would therefore hope for each thread to have the ability to calculate some index unique to itself between `0` and `99`.\n",
        "\n",
        "- If block `blockIdx.x` equals `9`, then `blockIdx.x * blockDim.x` is `90`. Adding to `90` the possible `threadIdx.x` values `0` through `9`, then we can generate the indices `90` through `99` within the 100 thread grid."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZpGzuqNtrxGD",
        "outputId": "4def993f-91c5-48f4-b4ec-f81bdd703bc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# using the absolute index of a thread\n",
        "!nvcc -o multi-block-loop 02-multi-block-loop.cu -run"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WmplOMr_rxGG"
      },
      "source": [
        "---\n",
        "## Allocating Memory to be accessed on the GPU and the CPU\n",
        "\n",
        "To allocate and free memory, and obtain a pointer that can be referenced in both host and device code, replace calls to `malloc` and `free` with `cudaMallocManaged` and `cudaFree` as in the following example:\n",
        "\n",
        "```cpp\n",
        "// CPU-only\n",
        "\n",
        "int N = 2<<20;\n",
        "size_t size = N * sizeof(int);\n",
        "\n",
        "int *a;\n",
        "a = (int *)malloc(size);\n",
        "\n",
        "// Use `a` in CPU-only program.\n",
        "\n",
        "free(a);\n",
        "```\n",
        "\n",
        "```cpp\n",
        "// Accelerated\n",
        "\n",
        "int N = 2<<20;\n",
        "size_t size = N * sizeof(int);\n",
        "\n",
        "int *a;\n",
        "// Note the address of `a` is passed as first argument.\n",
        "cudaMallocManaged(&a, size);\n",
        "\n",
        "// Use `a` on the CPU and/or on any GPU in the accelerated system.\n",
        "\n",
        "cudaFree(a);\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YiH3c6klrxGG"
      },
      "source": [
        "---\n",
        "### Exercise: Array Manipulation on both the Host and Device\n",
        "\n",
        "- `a` should be available to both host and device code.\n",
        "- The memory at `a` should be correctly freed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hz3S_1-rrxGH",
        "outputId": "5f5b14e9-923c-4c40-c58c-37ba62b802fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!nvcc -o double-elements 02-double-elements.cu -run"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All elements were doubled? TRUE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HL5QqbPVrxGJ"
      },
      "source": [
        "## Grid Size Work Amount Mismatch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bXXqEKrtrxGM"
      },
      "source": [
        "---\n",
        "## Handling Block Configuration Mismatches to Number of Needed Threads\n",
        "\n",
        "It may be the case that an execution configuration cannot be expressed that will create the exact number of threads needed for parallelizing a loop.\n",
        "\n",
        "A common example has to do with the desire to choose optimal block sizes. For example, due to GPU hardware traits, blocks that contain a number of threads that are a multiple of 32 are often desirable for performance benefits. Assuming that we wanted to launch blocks each containing 256 threads (a multiple of 32), and needed to run 1000 parallel tasks (a trivially small number for ease of explanation), then there is no number of blocks that would produce an exact total of 1000 threads in the grid, since there is no integer value 32 can be multiplied by to equal exactly 1000.\n",
        "\n",
        "This scenario can be easily addressed in the following way:\n",
        "\n",
        "- Write an execution configuration that creates **more** threads than necessary to perform the allotted work.\n",
        "- Pass a value as an argument into the kernel (`N`) that represents to the total size of the data set to be processed, or the total threads that are needed to complete the work.\n",
        "- After calculating the thread's index within the grid (using `tid+bid*bdim`), check that this index does not exceed `N`, and only perform the pertinent work of the kernel if it does not.\n",
        "\n",
        "Here is an example of an idiomatic way to write an execution configuration when both `N` and the number of threads in a block are known, and an exact match between the number of threads in the grid and `N` cannot be guaranteed. It ensures that there are always at least as many threads as needed for `N`, and only 1 additional block's worth of threads extra, at most:\n",
        "\n",
        "```cpp\n",
        "// Assume `N` is known\n",
        "int N = 100000;\n",
        "\n",
        "// Assume we have a desire to set `threads_per_block` exactly to `256`\n",
        "size_t threads_per_block = 256;\n",
        "\n",
        "// Ensure there are at least `N` threads in the grid, but only 1 block's worth extra\n",
        "size_t number_of_blocks = (N + threads_per_block - 1) / threads_per_block;\n",
        "\n",
        "some_kernel<<<number_of_blocks, threads_per_block>>>(N);\n",
        "```\n",
        "\n",
        "Becuase the execution configuration above results in more threads in the grid than `N`, care will need to be taken inside of the `some_kernel` definition so that `some_kernel` does not attempt to access out of range data elements, when being executed by one of the \"extra\" threads:\n",
        "\n",
        "```cpp\n",
        "__global__ some_kernel(int N)\n",
        "{\n",
        "  int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\n",
        "  if (idx < N) // Check to make sure `idx` maps to some value within `N`\n",
        "  {\n",
        "    // Only do work if it does\n",
        "  }\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bu3_GpcRrxGN",
        "outputId": "b7794de7-18f6-442b-fc70-2233ba79aaf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#launching 1000 tasks (not a multiple of 16) and executing the thread till 1000 (and not after)\n",
        "!nvcc -o mismatched-config-loop 02-mismatched-config-loop.cu -run"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SUCCESS!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XbtVrHHprxGP"
      },
      "source": [
        "---\n",
        "## Grid-Stride Loops\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_tQnMGrgrxGS"
      },
      "source": [
        "---\n",
        "## Data Sets Larger then the Grid\n",
        "\n",
        "Either by choice, often to create the most performant execution configuration, or out of necessity, the number of threads in a grid may be smaller than the size of a data set. Consider an array with 1000 elements, and a grid with 250 threads (using trivial sizes here for ease of explanation). Here, each thread in the grid will need to be used 4 times. One common method to do this is to use a **grid-stride loop** within the kernel.\n",
        "\n",
        "In a grid-stride loop, each thread will calculate its unique index within the grid using `tid+bid*bdim`, perform its operation on the element at that index within the array, and then, add to its index the number of threads in the grid and repeat, until it is out of range of the array. For example, for a 500 element array and a 250 thread grid, the thread with index 20 in the grid would:\n",
        "\n",
        "- Perform its operation on element 20 of the 500 element array\n",
        "- Increment its index by 250, the size of the grid, resulting in 270\n",
        "- Perform its operation on element 270 of the 500 element array\n",
        "- Increment its index by 250, the size of the grid, resulting in 520\n",
        "- Because 520 is now out of range for the array, the thread will stop its work\n",
        "\n",
        "CUDA provides a special variable giving the number of blocks in a grid, `gridDim.x`. Calculating the total number of threads in a grid then is simply the number of blocks in a grid multiplied by the number of threads in each block, `gridDim.x * blockDim.x`. With this in mind, here is a verbose example of a grid-stride loop within a kernel:\n",
        "\n",
        "```cpp\n",
        "__global void kernel(int *a, int N)\n",
        "{\n",
        "  int indexWithinTheGrid = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "  int gridStride = gridDim.x * blockDim.x;\n",
        "\n",
        "  for (int i = indexWithinTheGrid; i < N; i += gridStride)\n",
        "  {\n",
        "    // do work on a[i];\n",
        "  }\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xidW5BmYrxGT",
        "outputId": "321f1ad1-d4ae-400f-db5f-22ccd6166207",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#executing a code where the threads are less than the tasks to execute\n",
        "!nvcc -o grid-stride-double 03-grid-stride-double.cu -run"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All elements were doubled? TRUE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uRHinNberxGU"
      },
      "source": [
        "---\n",
        "## Error Handling\n",
        "\n",
        "As in any application, error handling in accelerated CUDA code is essential. Many, if not most CUDA functions (see, for example, the [memory management functions](http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY)) return a value of type `cudaError_t`, which can be used to check whether or not an error occured while calling the function. Here is an example where error handling is performed for a call to `cudaMallocManaged`:\n",
        "\n",
        "```cpp\n",
        "cudaError_t err;\n",
        "err = cudaMallocManaged(&a, N)                    // Assume the existence of `a` and `N`.\n",
        "\n",
        "if (err != cudaSuccess)                           // `cudaSuccess` is provided by CUDA.\n",
        "{\n",
        "  printf(\"Error: %s\\n\", cudaGetErrorString(err)); // `cudaGetErrorString` is provided by CUDA.\n",
        "}\n",
        "```\n",
        "\n",
        "Launching kernels, which are defined to return `void`, do not return a value of type `cudaError_t`. To check for errors occuring at the time of a kernel launch, for example if the launch configuration is erroneous, CUDA provides the `cudaGetLastError` function, which does return a value of type `cudaError_t`.\n",
        "\n",
        "```cpp\n",
        "/*\n",
        " * This launch should cause an error, but the kernel itself\n",
        " * cannot return it.\n",
        " */\n",
        "\n",
        "someKernel<<<1, -1>>>();  // -1 is not a valid number of threads.\n",
        "\n",
        "cudaError_t err;\n",
        "err = cudaGetLastError(); // `cudaGetLastError` will return the error from above.\n",
        "if (err != cudaSuccess)\n",
        "{\n",
        "  printf(\"Error: %s\\n\", cudaGetErrorString(err));\n",
        "}\n",
        "```\n",
        "\n",
        "Finally, in order to catch errors that occur asynchronously, for example during the execution of an asynchronous kernel, it is essential to check the status returned by a subsequent synchronizing cuda runtime API call, such as `cudaDeviceSynchronize`, which will return an error if one of the kernels launched previously should fail."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4etYGVMIrxGV",
        "outputId": "7505635e-fb51-48ec-e442-a3cfc5e561fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# checking for errors\n",
        "!nvcc -o add-error-handling 03-add-error-handling.cu -run"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All elements were doubled? TRUE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4FSFoAt1rxGX"
      },
      "source": [
        "---\n",
        "### CUDA Error Handling Function\n",
        "\n",
        "It can be helpful to create a macro that wraps CUDA function calls for checking errors. Here is an example, feel free to use it in the remaining exercises:\n",
        "\n",
        "```cpp\n",
        "#include <stdio.h>\n",
        "#include <assert.h>\n",
        "\n",
        "inline cudaError_t checkCuda(cudaError_t result)\n",
        "{\n",
        "  if (result != cudaSuccess) {\n",
        "    fprintf(stderr, \"CUDA Runtime Error: %s\\n\", cudaGetErrorString(result));\n",
        "    assert(result == cudaSuccess);\n",
        "  }\n",
        "  return result;\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "\n",
        "/*\n",
        " * The macro can be wrapped around any function returning\n",
        " * a value of type `cudaError_t`.\n",
        " */\n",
        "\n",
        "  checkCuda( cudaDeviceSynchronize() )\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Mk7DaaEQrxGY"
      },
      "source": [
        "---\n",
        "### Accelerate Vector Addition Application\n",
        "\n",
        "This application involves accelerating a CPU-only vector addition program"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u8peS-Z3rxGY",
        "outputId": "690d8e33-c3f1-45f6-ef1a-9e2f1685353a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!nvcc -o vector-add 05-vector-add.cu -run"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SUCCESS! All values added correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ELV91uZSrxGb"
      },
      "source": [
        "---\n",
        "## Grids and Blocks of 2 and 3 Dimensions\n",
        "\n",
        "Grids and blocks can be defined to have up to 3 dimensions. Defining them with multiple dimensions does not impact their performance in any way, but can be very helpful when dealing with data that has multiple dimensions, for example, 2d matrices. To define either grids or blocks with two or 3 dimensions, use CUDA's `dim3` type as such:\n",
        "\n",
        "```cpp\n",
        "dim3 threads_per_block(16, 16, 1);\n",
        "dim3 number_of_blocks(16, 16, 1);\n",
        "someKernel<<<number_of_blocks, threads_per_block>>>();\n",
        "```\n",
        "\n",
        "Given the example just above, the variables `gridDim.x`, `gridDim.y`, `blockDim.x`, and `blockDim.y` inside of `someKernel`, would all be equal to `16`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gnheeCczrxGb"
      },
      "source": [
        "---\n",
        "### Accelerate 2D Matrix Multiply Application"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YOHCXpVgrxGd",
        "outputId": "4c753854-f7c8-41b0-c12e-1b7e23f7d424",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!nvcc -o matrix-multiply-2d 06-matrix-multiply-2d.cu -run"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4exfWmFTrxGe"
      },
      "source": [
        "---\n",
        "### Accelerate A Thermal Conductivity Application\n",
        "\n",
        "This application simulates the thermal conduction of silver in 2 dimensional space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ljLX89mRrxGf",
        "outputId": "b1bab7b0-6f5a-4adf-b7b2-4b57ece5689e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!nvcc -o heat-conduction 07-heat-conduction.cu -run"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Max Error of 0.00001 is within acceptable bounds.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}