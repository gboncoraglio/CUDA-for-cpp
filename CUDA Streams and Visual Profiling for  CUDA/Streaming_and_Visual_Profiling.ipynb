{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Streaming and Visual Profiling.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "4R5SNgfOm9JW",
        "TdaA5Iwdm9JX",
        "H7FZknGJm9JX",
        "f4puCBgwm9Jx"
      ]
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLjJQqnYm9Ie",
        "colab_type": "text"
      },
      "source": [
        "<h1><div align=\"center\">Asynchronous Streaming, and Visual Profiling with CUDA C/C++</div></h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjL0U1aBnjij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks/CUDA for Cpp/CUDA Streams and Visual Profiling for  CUDA')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDmUt-nmm9Ig",
        "colab_type": "text"
      },
      "source": [
        "Nsight Systems generates a graphical timeline of an accelerated application, with detailed information about CUDA API calls, kernel execution, memory activity, and the use of **CUDA streams**.\n",
        "Nsight Systems timeline can be used to guide you in optimizing accelerated applications. \n",
        "\n",
        "Additionally, some intermediate CUDA programming techniques will be reviewed: **unmanaged memory allocation and migration**; **pinning**, or **page-locking** host memory; and **non-default concurrent CUDA streams**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QrWp2rSm9Ii",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Running Nsight Systems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF89qBBEm9Ij",
        "colab_type": "text"
      },
      "source": [
        "### Generate Report File\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwWxSkorm9Ij",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aa304d4b-0820-4125-ac54-8d998b109028"
      },
      "source": [
        "!nvcc -o vector-add-no-prefetch 01-vector-add.cu -run"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success! All values calculated correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-A2crnISm9Io",
        "colab_type": "text"
      },
      "source": [
        "Next, use `nsys profile --stats=true` to create a report file.\n",
        " Here we use the `-o` flag to give the report file a memorable name:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVZz7H97m9Io",
        "colab_type": "code",
        "colab": {},
        "outputId": "3eab4bb4-ff9b-4b03-bcc8-70699478db67"
      },
      "source": [
        "!nsys profile --stats=true -o vector-add-no-prefetch-report ./vector-add-no-prefetch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "**** collection configuration ****\n",
            "\toutput_filename = /dli/task/vector-add-no-prefetch-report\n",
            "\tforce-overwrite = false\n",
            "\tstop-on-exit = true\n",
            "\texport_sqlite = true\n",
            "\tstats = true\n",
            "\tcapture-range = none\n",
            "\tstop-on-range-end = false\n",
            "\tBeta: ftrace events:\n",
            "\tftrace-keep-user-config = false\n",
            "\ttrace-GPU-context-switch = false\n",
            "\tdelay = 0 seconds\n",
            "\tduration = 0 seconds\n",
            "\tkill = signal number 15\n",
            "\tinherit-environment = true\n",
            "\tshow-output = true\n",
            "\ttrace-fork-before-exec = false\n",
            "\tsample_cpu = true\n",
            "\tbacktrace_method = LBR\n",
            "\twait = all\n",
            "\ttrace_cublas = false\n",
            "\ttrace_cuda = true\n",
            "\ttrace_cudnn = false\n",
            "\ttrace_nvtx = true\n",
            "\ttrace_mpi = false\n",
            "\ttrace_openacc = false\n",
            "\ttrace_vulkan = false\n",
            "\ttrace_opengl = true\n",
            "\ttrace_osrt = true\n",
            "\tosrt-threshold = 0 nanoseconds\n",
            "\tcudabacktrace = false\n",
            "\tcudabacktrace-threshold = 0 nanoseconds\n",
            "\tprofile_processes = tree\n",
            "\tapplication command = ./vector-add-no-prefetch\n",
            "\tapplication arguments = \n",
            "\tapplication working directory = /dli/task\n",
            "\tNVTX profiler range trigger = \n",
            "\tNVTX profiler domain trigger = \n",
            "\tenvironment variables:\n",
            "\tCollecting data...\n",
            "Success! All values calculated correctly.\n",
            "\tGenerating the /dli/task/vector-add-no-prefetch-report.qdstrm file.\n",
            "\tCapturing raw events...\n",
            "\t10270 total events collected.\n",
            "\tSaving diagnostics...\n",
            "\tSaving qdstrm file to disk...\n",
            "\tFinished saving file.\n",
            "\n",
            "\n",
            "Importing the qdstrm file using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/QdstrmImporter.\n",
            "\n",
            "Importing...\n",
            "\n",
            "Importing [==================================================100%]\n",
            "Saving report to file \"/dli/task/vector-add-no-prefetch-report.qdrep\"\n",
            "Report file saved.\n",
            "Please discard the qdstrm file and use the qdrep file instead.\n",
            "\n",
            "Removed /dli/task/vector-add-no-prefetch-report.qdstrm as it was successfully imported.\n",
            "Please use the qdrep file instead.\n",
            "\n",
            "Exporting the qdrep file to SQLite database using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/nsys-exporter.\n",
            "\n",
            "Exporting 10228 events:\n",
            "\n",
            "0%   10   20   30   40   50   60   70   80   90   100%\n",
            "|----|----|----|----|----|----|----|----|----|----|\n",
            "***************************************************\n",
            "\n",
            "Exported successfully to\n",
            "/dli/task/vector-add-no-prefetch-report.sqlite\n",
            "\n",
            "Generating CUDA API Statistics...\n",
            "CUDA API Statistics (nanoseconds)\n",
            "\n",
            "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
            "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
            "   56.8       195841457           3      65280485.7           25930       195747936  cudaMallocManaged                                                               \n",
            "   36.3       125152808           1     125152808.0       125152808       125152808  cudaDeviceSynchronize                                                           \n",
            "    6.9        23762401           3       7920800.3         7286794         9098973  cudaFree                                                                        \n",
            "    0.0           67991           1         67991.0           67991           67991  cudaLaunchKernel                                                                \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Generating CUDA Kernel Statistics...\n",
            "\n",
            "Generating CUDA Memory Operation Statistics...\n",
            "CUDA Kernel Statistics (nanoseconds)\n",
            "\n",
            "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                            \n",
            "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
            "  100.0       125144482           1     125144482.0       125144482       125144482  addVectorsInto                                                                  \n",
            "\n",
            "\n",
            "CUDA Memory Operation Statistics (nanoseconds)\n",
            "\n",
            "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
            "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
            "   81.3        48661344        8191          5940.8            2848           96128  [CUDA Unified Memory memcpy HtoD]                                               \n",
            "   18.7        11168032         756         14772.5            1856           82496  [CUDA Unified Memory memcpy DtoH]                                               \n",
            "\n",
            "\n",
            "CUDA Memory Operation Statistics (KiB)\n",
            "\n",
            "            Total      Operations            Average            Minimum            Maximum  Name                                                                            \n",
            "-----------------  --------------  -----------------  -----------------  -----------------  --------------------------------------------------------------------------------\n",
            "         294960.0            8191               36.0              4.000             1008.0  [CUDA Unified Memory memcpy HtoD]                                               \n",
            "         129024.0             756              170.7              4.000             1020.0  [CUDA Unified Memory memcpy DtoH]                                               \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Generating Operating System Runtime API Statistics...\n",
            "Operating System Runtime API Statistics (nanoseconds)\n",
            "\n",
            "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
            "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
            "   55.1      1539741730          83      18551105.2            2339       100189977  poll                                                                            \n",
            "   41.2      1150661489          83      13863391.4           20122       100132697  sem_timedwait                                                                   \n",
            "    2.6        73819786         578        127715.9            1001        15837775  ioctl                                                                           \n",
            "    0.9        26011944          87        298987.9            1751         9004858  mmap                                                                            \n",
            "    0.0          694808          96          7237.6            1724           23850  fopen                                                                           \n",
            "    0.0          530015          78          6795.1            3917           15428  open64                                                                          \n",
            "    0.0          260228          89          2923.9            1559            5738  fclose                                                                          \n",
            "    0.0          141556           4         35389.0           33636           37454  pthread_create                                                                  \n",
            "    0.0          116403          10         11640.3            7010           16836  write                                                                           \n",
            "    0.0           93118           3         31039.3           24782           42359  fgets                                                                           \n",
            "    0.0           88078          77          1143.9            1004            5397  fcntl                                                                           \n",
            "    0.0           47540          12          3961.7            1984            6823  munmap                                                                          \n",
            "    0.0           38062           5          7612.4            4515           11274  open                                                                            \n",
            "    0.0           30369          12          2530.8            1318            5588  read                                                                            \n",
            "    0.0           16939           3          5646.3            4871            6279  pipe2                                                                           \n",
            "    0.0           10694           2          5347.0            4725            5969  socket                                                                          \n",
            "    0.0            9635           2          4817.5            4585            5050  fread                                                                           \n",
            "    0.0            8893           4          2223.3            2060            2304  mprotect                                                                        \n",
            "    0.0            8710           1          8710.0            8710            8710  connect                                                                         \n",
            "    0.0            8070           1          8070.0            8070            8070  pthread_cond_broadcast                                                          \n",
            "    0.0            2721           1          2721.0            2721            2721  bind                                                                            \n",
            "    0.0            2050           1          2050.0            2050            2050  listen                                                                          \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Generating NVTX Push-Pop Range Statistics...\n",
            "NVTX Push-Pop Range Statistics (nanoseconds)\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyOrtTKam9I_",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Comparing Code Refactors Iteratively"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIjG0xukm9JA",
        "colab_type": "text"
      },
      "source": [
        "### Compare the Timelines of Prefetching vs. Non-Prefetching\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wLA6uxLm9JB",
        "colab_type": "code",
        "colab": {},
        "outputId": "18d5ba24-5c66-42d7-b649-32e73acb0526"
      },
      "source": [
        "!nvcc -o vector-add-prefetch 01-vector-add/solutions/01-vector-add-prefetch-solution.cu -run"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success! All values calculated correctly.\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBrFP0aLm9JD",
        "colab_type": "text"
      },
      "source": [
        "Now create a report file for this version of the application:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0sINGRGm9JE",
        "colab_type": "code",
        "colab": {},
        "outputId": "759b2f99-8bf4-46b4-a36c-cfcce66ff25d"
      },
      "source": [
        "!nsys profile --stats=true -o vector-add-prefetch-report ./vector-add-prefetch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "**** collection configuration ****\n",
            "\toutput_filename = /dli/task/vector-add-prefetch-report\n",
            "\tforce-overwrite = false\n",
            "\tstop-on-exit = true\n",
            "\texport_sqlite = true\n",
            "\tstats = true\n",
            "\tcapture-range = none\n",
            "\tstop-on-range-end = false\n",
            "\tBeta: ftrace events:\n",
            "\tftrace-keep-user-config = false\n",
            "\ttrace-GPU-context-switch = false\n",
            "\tdelay = 0 seconds\n",
            "\tduration = 0 seconds\n",
            "\tkill = signal number 15\n",
            "\tinherit-environment = true\n",
            "\tshow-output = true\n",
            "\ttrace-fork-before-exec = false\n",
            "\tsample_cpu = true\n",
            "\tbacktrace_method = LBR\n",
            "\twait = all\n",
            "\ttrace_cublas = false\n",
            "\ttrace_cuda = true\n",
            "\ttrace_cudnn = false\n",
            "\ttrace_nvtx = true\n",
            "\ttrace_mpi = false\n",
            "\ttrace_openacc = false\n",
            "\ttrace_vulkan = false\n",
            "\ttrace_opengl = true\n",
            "\ttrace_osrt = true\n",
            "\tosrt-threshold = 0 nanoseconds\n",
            "\tcudabacktrace = false\n",
            "\tcudabacktrace-threshold = 0 nanoseconds\n",
            "\tprofile_processes = tree\n",
            "\tapplication command = ./vector-add-prefetch\n",
            "\tapplication arguments = \n",
            "\tapplication working directory = /dli/task\n",
            "\tNVTX profiler range trigger = \n",
            "\tNVTX profiler domain trigger = \n",
            "\tenvironment variables:\n",
            "\tCollecting data...\n",
            "Success! All values calculated correctly.\n",
            "\tGenerating the /dli/task/vector-add-prefetch-report.qdstrm file.\n",
            "\tCapturing raw events...\n",
            "\t2294 total events collected.\n",
            "\tSaving diagnostics...\n",
            "\tSaving qdstrm file to disk...\n",
            "\tFinished saving file.\n",
            "\n",
            "\n",
            "Importing the qdstrm file using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/QdstrmImporter.\n",
            "\n",
            "Importing...\n",
            "\n",
            "Importing [==================================================100%]\n",
            "Saving report to file \"/dli/task/vector-add-prefetch-report.qdrep\"\n",
            "Report file saved.\n",
            "Please discard the qdstrm file and use the qdrep file instead.\n",
            "\n",
            "Removed /dli/task/vector-add-prefetch-report.qdstrm as it was successfully imported.\n",
            "Please use the qdrep file instead.\n",
            "\n",
            "Exporting the qdrep file to SQLite database using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/nsys-exporter.\n",
            "\n",
            "Exporting 2253 events:\n",
            "\n",
            "0%   10   20   30   40   50   60   70   80   90   100%\n",
            "|----|----|----|----|----|----|----|----|----|----|\n",
            "***************************************************\n",
            "\n",
            "Exported successfully to\n",
            "/dli/task/vector-add-prefetch-report.sqlite\n",
            "\n",
            "Generating CUDA API Statistics...\n",
            "CUDA API Statistics (nanoseconds)\n",
            "\n",
            "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
            "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
            "   75.6       196768923           3      65589641.0           26041       196645789  cudaMallocManaged                                                               \n",
            "   11.7        30463289           1      30463289.0        30463289        30463289  cudaDeviceSynchronize                                                           \n",
            "    8.7        22678283           3       7559427.7         6853260         8938692  cudaFree                                                                        \n",
            "    4.0        10307133           3       3435711.0            9444        10122339  cudaMemPrefetchAsync                                                            \n",
            "    0.0           46697           1         46697.0           46697           46697  cudaLaunchKernel                                                                \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Generating CUDA Kernel Statistics...\n",
            "\n",
            "Generating CUDA Memory Operation Statistics...\n",
            "CUDA Kernel Statistics (nanoseconds)\n",
            "\n",
            "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                            \n",
            "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
            "  100.0          506497           1        506497.0          506497          506497  addVectorsInto                                                                  \n",
            "\n",
            "\n",
            "CUDA Memory Operation Statistics (nanoseconds)\n",
            "\n",
            "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
            "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
            "   76.6        37245600         192        193987.5          192768          195424  [CUDA Unified Memory memcpy HtoD]                                               \n",
            "   23.4        11393792         768         14835.7            1952           82560  [CUDA Unified Memory memcpy DtoH]                                               \n",
            "\n",
            "\n",
            "CUDA Memory Operation Statistics (KiB)\n",
            "\n",
            "            Total      Operations            Average            Minimum            Maximum  Name                                                                            \n",
            "-----------------  --------------  -----------------  -----------------  -----------------  --------------------------------------------------------------------------------\n",
            "         393216.0             192             2048.0           2048.000             2048.0  [CUDA Unified Memory memcpy HtoD]                                               \n",
            "         131072.0             768              170.7              4.000             1020.0  [CUDA Unified Memory memcpy DtoH]                                               \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Generating Operating System Runtime API Statistics...\n",
            "Operating System Runtime API Statistics (nanoseconds)\n",
            "\n",
            "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
            "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
            "   53.6      1379045558          81      17025253.8            2893       100196226  poll                                                                            \n",
            "   41.0      1054337694          74      14247806.7           20874       100138224  sem_timedwait                                                                   \n",
            "    4.2       108019772         579        186562.6            1039        15723803  ioctl                                                                           \n",
            "    1.0        25124598          88        285506.8            1654         8885207  mmap                                                                            \n",
            "    0.1         3394067           4        848516.8           31573         3111519  sem_wait                                                                        \n",
            "    0.0          696679          96          7257.1            1754           22206  fopen                                                                           \n",
            "    0.0          660033           5        132006.6           33511          470887  pthread_create                                                                  \n",
            "    0.0          494354          78          6337.9            4346           15240  open64                                                                          \n",
            "    0.0          253719          89          2850.8            1546            5388  fclose                                                                          \n",
            "    0.0          141883          14         10134.5            1457           14271  write                                                                           \n",
            "    0.0           91222           3         30407.3           23865           41467  fgets                                                                           \n",
            "    0.0           90737          80          1134.2            1013            4496  fcntl                                                                           \n",
            "    0.0           43171          12          3597.6            2203            7549  munmap                                                                          \n",
            "    0.0           38732          16          2420.7            1332            4788  read                                                                            \n",
            "    0.0           33162           5          6632.4            4629            9084  open                                                                            \n",
            "    0.0           30839           5          6167.8            2359           20525  mprotect                                                                        \n",
            "    0.0           15630           3          5210.0            3979            6279  pipe2                                                                           \n",
            "    0.0           11739           2          5869.5            5016            6723  socket                                                                          \n",
            "    0.0            8211           1          8211.0            8211            8211  connect                                                                         \n",
            "    0.0            7338           2          3669.0            1020            6318  pthread_cond_broadcast                                                          \n",
            "    0.0            7106           2          3553.0            2880            4226  fread                                                                           \n",
            "    0.0            2505           1          2505.0            2505            2505  bind                                                                            \n",
            "    0.0            1845           1          1845.0            1845            1845  listen                                                                          \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Generating NVTX Push-Pop Range Statistics...\n",
            "NVTX Push-Pop Range Statistics (nanoseconds)\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJJBqlxdm9JG",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Profile Refactor with Launch Init in Kernel\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XitpaLn6m9JH",
        "colab_type": "code",
        "colab": {},
        "outputId": "9ccb3408-f972-4a97-dc69-1b957b0ca933"
      },
      "source": [
        "!nvcc -o init-kernel 02-init-kernel/solutions/01-init-kernel-solution.cu -run"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success! All values calculated correctly.\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaG7vtBQm9JJ",
        "colab_type": "text"
      },
      "source": [
        "Now create a report file for this version of the application:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ppeuzlk_m9JK",
        "colab_type": "code",
        "colab": {},
        "outputId": "679cb87b-7d20-4ae1-ba40-923a8fab7598"
      },
      "source": [
        "!nsys profile --stats=true -o init-kernel-report ./init-kernel"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "**** collection configuration ****\n",
            "\toutput_filename = /dli/task/init-kernel-report\n",
            "\tforce-overwrite = false\n",
            "\tstop-on-exit = true\n",
            "\texport_sqlite = true\n",
            "\tstats = true\n",
            "\tcapture-range = none\n",
            "\tstop-on-range-end = false\n",
            "\tBeta: ftrace events:\n",
            "\tftrace-keep-user-config = false\n",
            "\ttrace-GPU-context-switch = false\n",
            "\tdelay = 0 seconds\n",
            "\tduration = 0 seconds\n",
            "\tkill = signal number 15\n",
            "\tinherit-environment = true\n",
            "\tshow-output = true\n",
            "\ttrace-fork-before-exec = false\n",
            "\tsample_cpu = true\n",
            "\tbacktrace_method = LBR\n",
            "\twait = all\n",
            "\ttrace_cublas = false\n",
            "\ttrace_cuda = true\n",
            "\ttrace_cudnn = false\n",
            "\ttrace_nvtx = true\n",
            "\ttrace_mpi = false\n",
            "\ttrace_openacc = false\n",
            "\ttrace_vulkan = false\n",
            "\ttrace_opengl = true\n",
            "\ttrace_osrt = true\n",
            "\tosrt-threshold = 0 nanoseconds\n",
            "\tcudabacktrace = false\n",
            "\tcudabacktrace-threshold = 0 nanoseconds\n",
            "\tprofile_processes = tree\n",
            "\tapplication command = ./init-kernel\n",
            "\tapplication arguments = \n",
            "\tapplication working directory = /dli/task\n",
            "\tNVTX profiler range trigger = \n",
            "\tNVTX profiler domain trigger = \n",
            "\tenvironment variables:\n",
            "\tCollecting data...\n",
            "Success! All values calculated correctly.\n",
            "\tGenerating the /dli/task/init-kernel-report.qdstrm file.\n",
            "\tCapturing raw events...\n",
            "\t2013 total events collected.\n",
            "\tSaving diagnostics...\n",
            "\tSaving qdstrm file to disk...\n",
            "\tFinished saving file.\n",
            "\n",
            "\n",
            "Importing the qdstrm file using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/QdstrmImporter.\n",
            "\n",
            "Importing...\n",
            "\n",
            "Importing [==================================================100%]\n",
            "Saving report to file \"/dli/task/init-kernel-report.qdrep\"\n",
            "Report file saved.\n",
            "Please discard the qdstrm file and use the qdrep file instead.\n",
            "\n",
            "Removed /dli/task/init-kernel-report.qdstrm as it was successfully imported.\n",
            "Please use the qdrep file instead.\n",
            "\n",
            "Exporting the qdrep file to SQLite database using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/nsys-exporter.\n",
            "\n",
            "Exporting 1977 events:\n",
            "\n",
            "0%   10   20   30   40   50   60   70   80   90   100%\n",
            "|----|----|----|----|----|----|----|----|----|----|\n",
            "***************************************************\n",
            "\n",
            "Exported successfully to\n",
            "/dli/task/init-kernel-report.sqlite\n",
            "\n",
            "Generating CUDA API Statistics...\n",
            "CUDA API Statistics (nanoseconds)\n",
            "\n",
            "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
            "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
            "   89.3       196708339           3      65569446.3           25369       196601006  cudaMallocManaged                                                               \n",
            "    8.8        19292261           3       6430753.7         5180467         8790856  cudaFree                                                                        \n",
            "    1.5         3258965           1       3258965.0         3258965         3258965  cudaDeviceSynchronize                                                           \n",
            "    0.5          998540           3        332846.7           13665          854510  cudaMemPrefetchAsync                                                            \n",
            "    0.0           73121           4         18280.3            8340           43565  cudaLaunchKernel                                                                \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Generating CUDA Kernel Statistics...\n",
            "\n",
            "Generating CUDA Memory Operation Statistics...\n",
            "CUDA Kernel Statistics (nanoseconds)\n",
            "\n",
            "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                            \n",
            "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
            "   51.5          502545           1        502545.0          502545          502545  addVectorsInto                                                                  \n",
            "   48.5          472338           3        157446.0          154492          162971  initWith                                                                        \n",
            "\n",
            "\n",
            "CUDA Memory Operation Statistics (nanoseconds)\n",
            "\n",
            "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
            "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
            "  100.0        11380640         768         14818.5            1920           82720  [CUDA Unified Memory memcpy DtoH]                                               \n",
            "\n",
            "\n",
            "CUDA Memory Operation Statistics (KiB)\n",
            "\n",
            "            Total      Operations            Average            Minimum            Maximum  Name                                                                            \n",
            "-----------------  --------------  -----------------  -----------------  -----------------  --------------------------------------------------------------------------------\n",
            "         131072.0             768              170.7              4.000             1020.0  [CUDA Unified Memory memcpy DtoH]                                               \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Generating Operating System Runtime API Statistics...\n",
            "Operating System Runtime API Statistics (nanoseconds)\n",
            "\n",
            "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
            "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
            "   45.4       477984679          30      15932822.6           25161       100145183  sem_timedwait                                                                   \n",
            "   45.0       474172787          35      13547793.9            2615       100190542  poll                                                                            \n",
            "    7.2        75768883         581        130411.2            1024        15718228  ioctl                                                                           \n",
            "    2.1        21661426          88        246152.6            1555         8739492  mmap                                                                            \n",
            "    0.1          695856          96          7248.5            1748           24475  fopen                                                                           \n",
            "    0.1          623054           5        124610.8           33749          466461  pthread_create                                                                  \n",
            "    0.0          488291          78          6260.1            4016           13719  open64                                                                          \n",
            "    0.0          439796           4        109949.0           35544          248062  sem_wait                                                                        \n",
            "    0.0          253415          89          2847.4            1539            5385  fclose                                                                          \n",
            "    0.0          146390          14         10456.4            1386           17921  write                                                                           \n",
            "    0.0           94821           3         31607.0           23047           41948  fgets                                                                           \n",
            "    0.0           90460          79          1145.1            1002            6250  fcntl                                                                           \n",
            "    0.0           42715          16          2669.7            1258            4794  read                                                                            \n",
            "    0.0           40591          11          3690.1            2070            7342  munmap                                                                          \n",
            "    0.0           33464           5          6692.8            4570            9559  open                                                                            \n",
            "    0.0           16331           3          5443.7            4416            6414  pipe2                                                                           \n",
            "    0.0           11750           5          2350.0            2033            2679  mprotect                                                                        \n",
            "    0.0           11688           3          3896.0            2346            4726  fread                                                                           \n",
            "    0.0            9940           2          4970.0            4624            5316  socket                                                                          \n",
            "    0.0            7090           1          7090.0            7090            7090  connect                                                                         \n",
            "    0.0            6613           1          6613.0            6613            6613  pthread_cond_broadcast                                                          \n",
            "    0.0            2901           1          2901.0            2901            2901  bind                                                                            \n",
            "    0.0            1957           1          1957.0            1957            1957  listen                                                                          \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Generating NVTX Push-Pop Range Statistics...\n",
            "NVTX Push-Pop Range Statistics (nanoseconds)\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80DumTwwm9JN",
        "colab_type": "text"
      },
      "source": [
        "### Profile Refactor with Asynchronous Prefetch Back to the Host\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRUPcaGlm9JN",
        "colab_type": "code",
        "colab": {},
        "outputId": "85194bfb-b2ee-420e-bede-7ef7fe0d4b9c"
      },
      "source": [
        "!nvcc -o prefetch-to-host 04-prefetch-check/solutions/01-prefetch-check-solution.cu -run"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success! All values calculated correctly.\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhkaWDx3m9JQ",
        "colab_type": "text"
      },
      "source": [
        "Now create a report file for this version of the application:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIUXkjHhm9JQ",
        "colab_type": "code",
        "colab": {},
        "outputId": "23c63023-9c52-4b91-d8db-40ac54e25c5a"
      },
      "source": [
        "!nsys profile --stats=true -o prefetch-to-host-report ./prefetch-to-host"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "**** collection configuration ****\n",
            "\toutput_filename = /dli/task/prefetch-to-host-report\n",
            "\tforce-overwrite = false\n",
            "\tstop-on-exit = true\n",
            "\texport_sqlite = true\n",
            "\tstats = true\n",
            "\tcapture-range = none\n",
            "\tstop-on-range-end = false\n",
            "\tBeta: ftrace events:\n",
            "\tftrace-keep-user-config = false\n",
            "\ttrace-GPU-context-switch = false\n",
            "\tdelay = 0 seconds\n",
            "\tduration = 0 seconds\n",
            "\tkill = signal number 15\n",
            "\tinherit-environment = true\n",
            "\tshow-output = true\n",
            "\ttrace-fork-before-exec = false\n",
            "\tsample_cpu = true\n",
            "\tbacktrace_method = LBR\n",
            "\twait = all\n",
            "\ttrace_cublas = false\n",
            "\ttrace_cuda = true\n",
            "\ttrace_cudnn = false\n",
            "\ttrace_nvtx = true\n",
            "\ttrace_mpi = false\n",
            "\ttrace_openacc = false\n",
            "\ttrace_vulkan = false\n",
            "\ttrace_opengl = true\n",
            "\ttrace_osrt = true\n",
            "\tosrt-threshold = 0 nanoseconds\n",
            "\tcudabacktrace = false\n",
            "\tcudabacktrace-threshold = 0 nanoseconds\n",
            "\tprofile_processes = tree\n",
            "\tapplication command = ./prefetch-to-host\n",
            "\tapplication arguments = \n",
            "\tapplication working directory = /dli/task\n",
            "\tNVTX profiler range trigger = \n",
            "\tNVTX profiler domain trigger = \n",
            "\tenvironment variables:\n",
            "\tCollecting data...\n",
            "Success! All values calculated correctly.\n",
            "\tGenerating the /dli/task/prefetch-to-host-report.qdstrm file.\n",
            "\tCapturing raw events...\n",
            "\t1305 total events collected.\n",
            "\tSaving diagnostics...\n",
            "\tSaving qdstrm file to disk...\n",
            "\tFinished saving file.\n",
            "\n",
            "\n",
            "Importing the qdstrm file using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/QdstrmImporter.\n",
            "\n",
            "Importing...\n",
            "\n",
            "Importing [==================================================100%]\n",
            "Saving report to file \"/dli/task/prefetch-to-host-report.qdrep\"\n",
            "Report file saved.\n",
            "Please discard the qdstrm file and use the qdrep file instead.\n",
            "\n",
            "Removed /dli/task/prefetch-to-host-report.qdstrm as it was successfully imported.\n",
            "Please use the qdrep file instead.\n",
            "\n",
            "Exporting the qdrep file to SQLite database using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/nsys-exporter.\n",
            "\n",
            "Exporting 1269 events:\n",
            "\n",
            "0%   10   20   30   40   50   60   70   80   90   100%\n",
            "|----|----|----|----|----|----|----|----|----|----|\n",
            "***************************************************\n",
            "\n",
            "Exported successfully to\n",
            "/dli/task/prefetch-to-host-report.sqlite\n",
            "\n",
            "Generating CUDA API Statistics...\n",
            "CUDA API Statistics (nanoseconds)\n",
            "\n",
            "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
            "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
            "   82.7       201475554           3      67158518.0           24878       201403342  cudaMallocManaged                                                               \n",
            "    8.3        20169642           4       5042410.5           11370        19190602  cudaMemPrefetchAsync                                                            \n",
            "    7.6        18623416           3       6207805.3         5176259         8118690  cudaFree                                                                        \n",
            "    1.4         3293321           1       3293321.0         3293321         3293321  cudaDeviceSynchronize                                                           \n",
            "    0.0           64845           4         16211.2            7599           37640  cudaLaunchKernel                                                                \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Generating CUDA Kernel Statistics...\n",
            "\n",
            "Generating CUDA Memory Operation Statistics...\n",
            "CUDA Kernel Statistics (nanoseconds)\n",
            "\n",
            "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                            \n",
            "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
            "   51.7          505558           1        505558.0          505558          505558  addVectorsInto                                                                  \n",
            "   48.3          472374           3        157458.0          154045          161789  initWith                                                                        \n",
            "\n",
            "\n",
            "CUDA Memory Operation Statistics (nanoseconds)\n",
            "\n",
            "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
            "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
            "  100.0        10650176          64        166409.0          166176          167136  [CUDA Unified Memory memcpy DtoH]                                               \n",
            "\n",
            "\n",
            "CUDA Memory Operation Statistics (KiB)\n",
            "\n",
            "            Total      Operations            Average            Minimum            Maximum  Name                                                                            \n",
            "-----------------  --------------  -----------------  -----------------  -----------------  --------------------------------------------------------------------------------\n",
            "         131072.0              64             2048.0           2048.000             2048.0  [CUDA Unified Memory memcpy DtoH]                                               \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Generating Operating System Runtime API Statistics...\n",
            "Operating System Runtime API Statistics (nanoseconds)\n",
            "\n",
            "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
            "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
            "   44.4       432675970          31      13957289.4            2557       100179469  poll                                                                            \n",
            "   43.2       421036120          26      16193696.9           21430       100118227  sem_timedwait                                                                   \n",
            "   10.0        97282277         582        167151.7            1011        19142339  ioctl                                                                           \n",
            "    2.2        21010603          88        238756.9            1682         8066913  mmap                                                                            \n",
            "    0.1          726496          96          7567.7            1825           21496  fopen                                                                           \n",
            "    0.1          659839           5        131967.8           34652          503272  pthread_create                                                                  \n",
            "    0.1          566912          78          7268.1            4391           17170  open64                                                                          \n",
            "    0.0          400442           3        133480.7           52516          258322  sem_wait                                                                        \n",
            "    0.0          266420          89          2993.5            1509            5181  fclose                                                                          \n",
            "    0.0          152300          14         10878.6            1477           17016  write                                                                           \n",
            "    0.0           93892           3         31297.3           24865           42539  fgets                                                                           \n",
            "    0.0           89924          79          1138.3            1008            5273  fcntl                                                                           \n",
            "    0.0           42302          13          3254.0            1507            4774  munmap                                                                          \n",
            "    0.0           41422           5          8284.4            4768           11638  open                                                                            \n",
            "    0.0           37122          16          2320.1            1262            4120  read                                                                            \n",
            "    0.0           17314           3          5771.3            4255            6605  pipe2                                                                           \n",
            "    0.0           14513           2          7256.5            5761            8752  socket                                                                          \n",
            "    0.0           12326           5          2465.2            2288            2646  mprotect                                                                        \n",
            "    0.0            9707           1          9707.0            9707            9707  connect                                                                         \n",
            "    0.0            9090           3          3030.0            1947            4205  fread                                                                           \n",
            "    0.0            7945           2          3972.5            1012            6933  pthread_cond_broadcast                                                          \n",
            "    0.0            3154           1          3154.0            3154            3154  bind                                                                            \n",
            "    0.0            2381           1          2381.0            2381            2381  listen                                                                          \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Generating NVTX Push-Pop Range Statistics...\n",
            "NVTX Push-Pop Range Statistics (nanoseconds)\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X88XD9u8m9JT",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Concurrent CUDA Streams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ornk8YEpm9JW",
        "colab_type": "text"
      },
      "source": [
        "In CUDA programming, a **stream** is a series of commands that execute in order. In CUDA applications, kernel execution, as well as some memory transfers, occur within CUDA streams.\n",
        "\n",
        "CUDA programmers can create and utilize non-default CUDA streams in addition to the default stream, and in doing so, perform multiple operations, such as executing multiple kernels, concurrently, in different streams. Using multiple streams can add an additional layer of parallelization to your accelerated applications, and offers many more opportunities for application optimization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4R5SNgfOm9JW",
        "colab_type": "text"
      },
      "source": [
        "### Rules Governing the Behavior of CUDA Streams\n",
        "\n",
        "There are a few rules, concerning the behavior of CUDA streams, that should be learned in order to utilize them effectively:\n",
        "\n",
        "- Operations within a given stream occur in order.\n",
        "- Operations in different non-default streams are not guaranteed to operate in any specific order relative to each other.\n",
        "- The default stream is blocking and will both wait for all other streams to complete before running, and, will block other streams from running until it completes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdaA5Iwdm9JX",
        "colab_type": "text"
      },
      "source": [
        "### Creating, Utilizing, and Destroying Non-Default CUDA Streams\n",
        "\n",
        "The following code snippet demonstrates how to create, utilize, and destroy a non-default CUDA stream. You will note, that to launch a CUDA kernel in a non-default CUDA stream, the stream must be passed as the optional 4th argument of the execution configuration. Up until now you have only utilized the first 2 arguments of the execution configuration:\n",
        "\n",
        "```cpp\n",
        "cudaStream_t stream;       // CUDA streams are of type `cudaStream_t`.\n",
        "cudaStreamCreate(&stream); // Note that a pointer must be passed to `cudaCreateStream`.\n",
        "\n",
        "someKernel<<<number_of_blocks, threads_per_block, 0, stream>>>(); // `stream` is passed as 4th EC argument.\n",
        "\n",
        "cudaStreamDestroy(stream); // Note that a value, not a pointer, is passed to `cudaDestroyStream`.\n",
        "```\n",
        "\n",
        "The optional 3rd argument of the execution configuration. This argument allows programmers to supply the number of bytes in **shared memory** (an advanced topic that will not be covered presently) to be dynamically allocated per block for this kernel launch. The default number of bytes allocated to shared memory per block is `0`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7FZknGJm9JX",
        "colab_type": "text"
      },
      "source": [
        "### Predict Default Stream Behavior\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6In6iATm9JX",
        "colab_type": "code",
        "colab": {},
        "outputId": "57d623b1-bda9-4a3b-a971-3149997d1705"
      },
      "source": [
        "!nvcc -o print-numbers 05-stream-intro/01-print-numbers.cu -run"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\r\n",
            "1\r\n",
            "2\r\n",
            "3\r\n",
            "4\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oWjcM0wm9Ja",
        "colab_type": "text"
      },
      "source": [
        "Knowing that by default kernels are executed in the default stream, would you expect that the 5 launches of the `print-numbers` program executed serially, or in parallel? You should be able to mention two features of the default stream to support your answer. Create a report file in the cell below and open it in Nsight Systems to confirm your answer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKIOxwCtm9Ja",
        "colab_type": "code",
        "colab": {},
        "outputId": "72f657a6-2958-49db-b3e2-eb3c6d769605"
      },
      "source": [
        "!nsys profile --stats=true -o print-numbers-report ./print-numbers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "**** collection configuration ****\n",
            "\toutput_filename = /dli/task/print-numbers-report\n",
            "\tforce-overwrite = false\n",
            "\tstop-on-exit = true\n",
            "\texport_sqlite = true\n",
            "\tstats = true\n",
            "\tcapture-range = none\n",
            "\tstop-on-range-end = false\n",
            "\tBeta: ftrace events:\n",
            "\tftrace-keep-user-config = false\n",
            "\ttrace-GPU-context-switch = false\n",
            "\tdelay = 0 seconds\n",
            "\tduration = 0 seconds\n",
            "\tkill = signal number 15\n",
            "\tinherit-environment = true\n",
            "\tshow-output = true\n",
            "\ttrace-fork-before-exec = false\n",
            "\tsample_cpu = true\n",
            "\tbacktrace_method = LBR\n",
            "\twait = all\n",
            "\ttrace_cublas = false\n",
            "\ttrace_cuda = true\n",
            "\ttrace_cudnn = false\n",
            "\ttrace_nvtx = true\n",
            "\ttrace_mpi = false\n",
            "\ttrace_openacc = false\n",
            "\ttrace_vulkan = false\n",
            "\ttrace_opengl = true\n",
            "\ttrace_osrt = true\n",
            "\tosrt-threshold = 0 nanoseconds\n",
            "\tcudabacktrace = false\n",
            "\tcudabacktrace-threshold = 0 nanoseconds\n",
            "\tprofile_processes = tree\n",
            "\tapplication command = ./print-numbers\n",
            "\tapplication arguments = \n",
            "\tapplication working directory = /dli/task\n",
            "\tNVTX profiler range trigger = \n",
            "\tNVTX profiler domain trigger = \n",
            "\tenvironment variables:\n",
            "\tCollecting data...\n",
            "The application process terminated. One or more process it created re-parented. Waiting for termination of re-parented processes. To modify this behavior, use the `--wait` option.\n",
            "\u00000\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "\tGenerating the /dli/task/print-numbers-report.qdstrm file.\n",
            "\tCapturing raw events...\n",
            "\t1158 total events collected.\n",
            "\tSaving diagnostics...\n",
            "\tSaving qdstrm file to disk...\n",
            "\tFinished saving file.\n",
            "\n",
            "\n",
            "Importing the qdstrm file using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/QdstrmImporter.\n",
            "\n",
            "Importing...\n",
            "\n",
            "Importing [==================================================100%]\n",
            "Saving report to file \"/dli/task/print-numbers-report.qdrep\"\n",
            "Report file saved.\n",
            "Please discard the qdstrm file and use the qdrep file instead.\n",
            "\n",
            "Removed /dli/task/print-numbers-report.qdstrm as it was successfully imported.\n",
            "Please use the qdrep file instead.\n",
            "\n",
            "Exporting the qdrep file to SQLite database using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/nsys-exporter.\n",
            "\n",
            "Exporting 1123 events:\n",
            "\n",
            "0%   10   20   30   40   50   60   70   80   90   100%\n",
            "|----|----|----|----|----|----|----|----|----|----|\n",
            "***************************************************\n",
            "\n",
            "Exported successfully to\n",
            "/dli/task/print-numbers-report.sqlite\n",
            "\n",
            "Generating CUDA API Statistics...\n",
            "CUDA API Statistics (nanoseconds)\n",
            "\n",
            "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
            "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
            "   99.9       178160191           5      35632038.2            6924       178126220  cudaLaunchKernel                                                                \n",
            "    0.1          153649           1        153649.0          153649          153649  cudaDeviceSynchronize                                                           \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Generating CUDA Kernel Statistics...\n",
            "CUDA Kernel Statistics (nanoseconds)\n",
            "\n",
            "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                            \n",
            "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
            "  100.0          175160           5         35032.0           32837           42662  printNumber                                                                     \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Generating Operating System Runtime API Statistics...\n",
            "Operating System Runtime API Statistics (nanoseconds)\n",
            "\n",
            "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
            "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
            "   44.7       210203691          13      16169514.7           23786       100120506  sem_timedwait                                                                   \n",
            "   37.7       177296357          12      14774696.4           38170        77364941  poll                                                                            \n",
            "   16.5        77385342         563        137451.8            1032        15420546  ioctl                                                                           \n",
            "    0.6         2585526          80         32319.1            1643          773196  mmap                                                                            \n",
            "    0.1          666669          97          6872.9            1750           14957  fopen                                                                           \n",
            "    0.1          587368           4        146842.0           32950          478493  pthread_create                                                                  \n",
            "    0.1          518177          79          6559.2            4039           12372  open64                                                                          \n",
            "    0.1          251637          90          2796.0            1603            3993  fclose                                                                          \n",
            "    0.0          120957          11         10996.1            7234           15125  write                                                                           \n",
            "    0.0           92760           3         30920.0           24135           41397  fgets                                                                           \n",
            "    0.0           87547          78          1122.4            1016            4292  fcntl                                                                           \n",
            "    0.0           31757          13          2442.8            1448            4011  read                                                                            \n",
            "    0.0           31326           5          6265.2            4439            8962  open                                                                            \n",
            "    0.0           27199           8          3399.9            1586            5011  munmap                                                                          \n",
            "    0.0           14626           3          4875.3            4125            5362  pipe2                                                                           \n",
            "    0.0           11329           2          5664.5            4876            6453  socket                                                                          \n",
            "    0.0            8975           4          2243.8            2074            2492  mprotect                                                                        \n",
            "    0.0            7663           1          7663.0            7663            7663  connect                                                                         \n",
            "    0.0            6960           1          6960.0            6960            6960  pthread_cond_broadcast                                                          \n",
            "    0.0            6721           2          3360.5            2657            4064  fread                                                                           \n",
            "    0.0            4586           1          4586.0            4586            4586  fflush                                                                          \n",
            "    0.0            2560           1          2560.0            2560            2560  bind                                                                            \n",
            "    0.0            1822           1          1822.0            1822            1822  listen                                                                          \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Generating NVTX Push-Pop Range Statistics...\n",
            "NVTX Push-Pop Range Statistics (nanoseconds)\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCnYsnYDm9Jc",
        "colab_type": "text"
      },
      "source": [
        "### Implement Concurrent CUDA Streams\n",
        "\n",
        "Both because all 5 kernel launches occured in the same stream, you should not be surprised to have seen that the 5 kernels executed serially. Additionally you could make the case that because the default stream is blocking, each launch of the kernel would wait to complete before the next launch, and this is also true.\n",
        "\n",
        "Refactor the code so that each kernel launch occurs in its own non-default stream. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjpTCDAlm9Jd",
        "colab_type": "code",
        "colab": {},
        "outputId": "bfc3f1e2-9462-472e-c8f3-2b29f91f1774"
      },
      "source": [
        "!nvcc -o print-numbers-in-streams 05-stream-intro/01-print-numbers.cu -run"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\r\n",
            "1\r\n",
            "2\r\n",
            "3\r\n",
            "4\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEoYWgAHm9Jg",
        "colab_type": "code",
        "colab": {},
        "outputId": "a56e0984-5f60-407f-d9ea-a204e1e77726"
      },
      "source": [
        "!nsys profile --stats=true -o print-numbers-in-streams-report print-numbers-in-streams"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "**** collection configuration ****\n",
            "\toutput_filename = /dli/task/print-numbers-in-streams-report\n",
            "\tforce-overwrite = false\n",
            "\tstop-on-exit = true\n",
            "\texport_sqlite = true\n",
            "\tstats = true\n",
            "\tcapture-range = none\n",
            "\tstop-on-range-end = false\n",
            "\tBeta: ftrace events:\n",
            "\tftrace-keep-user-config = false\n",
            "\ttrace-GPU-context-switch = false\n",
            "\tdelay = 0 seconds\n",
            "\tduration = 0 seconds\n",
            "\tkill = signal number 15\n",
            "\tinherit-environment = true\n",
            "\tshow-output = true\n",
            "\ttrace-fork-before-exec = false\n",
            "\tsample_cpu = true\n",
            "\tbacktrace_method = LBR\n",
            "\twait = all\n",
            "\ttrace_cublas = false\n",
            "\ttrace_cuda = true\n",
            "\ttrace_cudnn = false\n",
            "\ttrace_nvtx = true\n",
            "\ttrace_mpi = false\n",
            "\ttrace_openacc = false\n",
            "\ttrace_vulkan = false\n",
            "\ttrace_opengl = true\n",
            "\ttrace_osrt = true\n",
            "\tosrt-threshold = 0 nanoseconds\n",
            "\tcudabacktrace = false\n",
            "\tcudabacktrace-threshold = 0 nanoseconds\n",
            "\tprofile_processes = tree\n",
            "\tapplication command = print-numbers-in-streams\n",
            "\tapplication arguments = \n",
            "\tapplication working directory = /dli/task\n",
            "\tNVTX profiler range trigger = \n",
            "\tNVTX profiler domain trigger = \n",
            "\tenvironment variables:\n",
            "\tCollecting data...\n",
            "The application process terminated. One or more process it created re-parented. Waiting for termination of re-parented processes. To modify this behavior, use the `--wait` option.\n",
            "\u00000\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "\tGenerating the /dli/task/print-numbers-in-streams-report.qdstrm file.\n",
            "\tCapturing raw events...\n",
            "\t1189 total events collected.\n",
            "\tSaving diagnostics...\n",
            "\tSaving qdstrm file to disk...\n",
            "\tFinished saving file.\n",
            "\n",
            "\n",
            "Importing the qdstrm file using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/QdstrmImporter.\n",
            "\n",
            "Importing...\n",
            "\n",
            "Importing [==================================================100%]\n",
            "Saving report to file \"/dli/task/print-numbers-in-streams-report.qdrep\"\n",
            "Report file saved.\n",
            "Please discard the qdstrm file and use the qdrep file instead.\n",
            "\n",
            "Removed /dli/task/print-numbers-in-streams-report.qdstrm as it was successfully imported.\n",
            "Please use the qdrep file instead.\n",
            "\n",
            "Exporting the qdrep file to SQLite database using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/nsys-exporter.\n",
            "\n",
            "Exporting 1149 events:\n",
            "\n",
            "0%   10   20   30   40   50   60   70   80   90   100%\n",
            "|----|----|----|----|----|----|----|----|----|----|\n",
            "***************************************************\n",
            "\n",
            "Exported successfully to\n",
            "/dli/task/print-numbers-in-streams-report.sqlite\n",
            "\n",
            "Generating CUDA API Statistics...\n",
            "CUDA API Statistics (nanoseconds)\n",
            "\n",
            "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
            "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
            "   99.9       179972667           5      35994533.4            4117       179943689  cudaStreamCreate                                                                \n",
            "    0.1          103454           5         20690.8            9453           60282  cudaLaunchKernel                                                                \n",
            "    0.0           46621           1         46621.0           46621           46621  cudaDeviceSynchronize                                                           \n",
            "    0.0           42194           5          8438.8            5223           16311  cudaStreamDestroy                                                               \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Generating CUDA Kernel Statistics...\n",
            "CUDA Kernel Statistics (nanoseconds)\n",
            "\n",
            "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                            \n",
            "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
            "  100.0          196091           5         39218.2           33125           48775  printNumber                                                                     \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Generating Operating System Runtime API Statistics...\n",
            "Operating System Runtime API Statistics (nanoseconds)\n",
            "\n",
            "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
            "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
            "   44.7       211845421          13      16295801.6           24053       100122430  sem_timedwait                                                                   \n",
            "   37.7       178723327          12      14893610.6           38610        78086880  poll                                                                            \n",
            "   16.5        77972492         575        135604.3            1020        15488607  ioctl                                                                           \n",
            "    0.5         2600325          80         32504.1            1810          775573  mmap                                                                            \n",
            "    0.1          673690          97          6945.3            1718           15900  fopen                                                                           \n",
            "    0.1          585550           4        146387.5           31782          479435  pthread_create                                                                  \n",
            "    0.1          485779          79          6149.1            4096           11810  open64                                                                          \n",
            "    0.1          258556          90          2872.8            1590            3880  fclose                                                                          \n",
            "    0.0          121285          11         11025.9            6976           13927  write                                                                           \n",
            "    0.0           91795          81          1133.3            1001            4990  fcntl                                                                           \n",
            "    0.0           91122           3         30374.0           23614           41389  fgets                                                                           \n",
            "    0.0           32397           1         32397.0           32397           32397  sem_wait                                                                        \n",
            "    0.0           30059          13          2312.2            1370            4320  read                                                                            \n",
            "    0.0           29836           5          5967.2            4442            8086  open                                                                            \n",
            "    0.0           24851           7          3550.1            2296            4840  munmap                                                                          \n",
            "    0.0           16795           3          5598.3            4686            6525  pipe2                                                                           \n",
            "    0.0           10758           2          5379.0            4945            5813  socket                                                                          \n",
            "    0.0            8985           4          2246.3            2077            2459  mprotect                                                                        \n",
            "    0.0            7214           1          7214.0            7214            7214  connect                                                                         \n",
            "    0.0            6849           2          3424.5            2756            4093  fread                                                                           \n",
            "    0.0            5948           1          5948.0            5948            5948  pthread_cond_broadcast                                                          \n",
            "    0.0            5355           2          2677.5            1208            4147  fflush                                                                          \n",
            "    0.0            2496           1          2496.0            2496            2496  bind                                                                            \n",
            "    0.0            1887           1          1887.0            1887            1887  listen                                                                          \n",
            "\n",
            "\n",
            "\n",
            "\r\n",
            "Generating NVTX Push-Pop Range Statistics...\r\n",
            "NVTX Push-Pop Range Statistics (nanoseconds)\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3W4RWAxm9Jj",
        "colab_type": "text"
      },
      "source": [
        "### Use Streams for Concurrent Data Initialization Kernels\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilO2hqYsm9Jj",
        "colab_type": "code",
        "colab": {},
        "outputId": "eaa1807d-1b8f-4f54-ee9b-5bd80055c558"
      },
      "source": [
        "!nvcc -o init-in-streams 04-prefetch-check/solutions/01-prefetch-check-solution.cu -run"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success! All values calculated correctly.\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "879qXgrNm9Jm",
        "colab_type": "code",
        "colab": {},
        "outputId": "7916ee25-a292-4807-dbdd-05283ddcbc2e"
      },
      "source": [
        "!nsys profile --stats=true -o init-in-streams-report ./init-in-streams"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "**** collection configuration ****\n",
            "\toutput_filename = /dli/task/init-in-streams-report\n",
            "\tforce-overwrite = false\n",
            "\tstop-on-exit = true\n",
            "\texport_sqlite = true\n",
            "\tstats = true\n",
            "\tcapture-range = none\n",
            "\tstop-on-range-end = false\n",
            "\tBeta: ftrace events:\n",
            "\tftrace-keep-user-config = false\n",
            "\ttrace-GPU-context-switch = false\n",
            "\tdelay = 0 seconds\n",
            "\tduration = 0 seconds\n",
            "\tkill = signal number 15\n",
            "\tinherit-environment = true\n",
            "\tshow-output = true\n",
            "\ttrace-fork-before-exec = false\n",
            "\tsample_cpu = true\n",
            "\tbacktrace_method = LBR\n",
            "\twait = all\n",
            "\ttrace_cublas = false\n",
            "\ttrace_cuda = true\n",
            "\ttrace_cudnn = false\n",
            "\ttrace_nvtx = true\n",
            "\ttrace_mpi = false\n",
            "\ttrace_openacc = false\n",
            "\ttrace_vulkan = false\n",
            "\ttrace_opengl = true\n",
            "\ttrace_osrt = true\n",
            "\tosrt-threshold = 0 nanoseconds\n",
            "\tcudabacktrace = false\n",
            "\tcudabacktrace-threshold = 0 nanoseconds\n",
            "\tprofile_processes = tree\n",
            "\tapplication command = ./init-in-streams\n",
            "\tapplication arguments = \n",
            "\tapplication working directory = /dli/task\n",
            "\tNVTX profiler range trigger = \n",
            "\tNVTX profiler domain trigger = \n",
            "\tenvironment variables:\n",
            "\tCollecting data...\n",
            "Success! All values calculated correctly.\n",
            "\tGenerating the /dli/task/init-in-streams-report.qdstrm file.\n",
            "\tCapturing raw events...\n",
            "\t1315 total events collected.\n",
            "\tSaving diagnostics...\n",
            "\tSaving qdstrm file to disk...\n",
            "\tFinished saving file.\n",
            "\n",
            "\n",
            "Importing the qdstrm file using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/QdstrmImporter.\n",
            "\n",
            "Importing...\n",
            "\n",
            "Importing [==================================================100%]\n",
            "Saving report to file \"/dli/task/init-in-streams-report.qdrep\"\n",
            "Report file saved.\n",
            "Please discard the qdstrm file and use the qdrep file instead.\n",
            "\n",
            "Removed /dli/task/init-in-streams-report.qdstrm as it was successfully imported.\n",
            "Please use the qdrep file instead.\n",
            "\n",
            "Exporting the qdrep file to SQLite database using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/nsys-exporter.\n",
            "\n",
            "Exporting 1276 events:\n",
            "\n",
            "0%   10   20   30   40   50   60   70   80   90   100%\n",
            "|----|----|----|----|----|----|----|----|----|----|\n",
            "***************************************************\n",
            "\n",
            "Exported successfully to\n",
            "/dli/task/init-in-streams-report.sqlite\n",
            "\n",
            "Generating CUDA API Statistics...\n",
            "CUDA API Statistics (nanoseconds)\n",
            "\n",
            "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
            "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
            "   82.4       196924269           3      65641423.0           24734       196846762  cudaMallocManaged                                                               \n",
            "    8.3        19942032           4       4985508.0           12031        18942056  cudaMemPrefetchAsync                                                            \n",
            "    7.8        18563750           3       6187916.7         5170132         8076503  cudaFree                                                                        \n",
            "    1.4         3274014           1       3274014.0         3274014         3274014  cudaDeviceSynchronize                                                           \n",
            "    0.0           73956           4         18489.0           10645           41276  cudaLaunchKernel                                                                \n",
            "    0.0           62185           3         20728.3            4837           50950  cudaStreamDestroy                                                               \n",
            "    0.0           24499           3          8166.3            4173           15903  cudaStreamCreate                                                                \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Generating CUDA Kernel Statistics...\n",
            "\n",
            "Generating CUDA Memory Operation Statistics...\n",
            "CUDA Kernel Statistics (nanoseconds)\n",
            "\n",
            "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                            \n",
            "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
            "   55.0          617265           3        205755.0          185564          228186  initWith                                                                        \n",
            "   45.0          504116           1        504116.0          504116          504116  addVectorsInto                                                                  \n",
            "\n",
            "\n",
            "CUDA Memory Operation Statistics (nanoseconds)\n",
            "\n",
            "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
            "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
            "  100.0        10678976          64        166859.0          166240          181888  [CUDA Unified Memory memcpy DtoH]                                               \n",
            "\n",
            "\n",
            "CUDA Memory Operation Statistics (KiB)\n",
            "\n",
            "            Total      Operations            Average            Minimum            Maximum  Name                                                                            \n",
            "-----------------  --------------  -----------------  -----------------  -----------------  --------------------------------------------------------------------------------\n",
            "         131072.0              64             2048.0           2048.000             2048.0  [CUDA Unified Memory memcpy DtoH]                                               \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Generating Operating System Runtime API Statistics...\n",
            "Operating System Runtime API Statistics (nanoseconds)\n",
            "\n",
            "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
            "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
            "   44.8       432943813          31      13965929.5            2647       100178141  poll                                                                            \n",
            "   42.9       414812445          26      15954324.8           20703       100133801  sem_timedwait                                                                   \n",
            "    9.8        94691643         584        162143.2            1053        18907420  ioctl                                                                           \n",
            "    2.2        20966824          88        238259.4            1499         8026892  mmap                                                                            \n",
            "    0.1          680231          96          7085.7            1786           21927  fopen                                                                           \n",
            "    0.1          640392           5        128078.4           35522          472480  pthread_create                                                                  \n",
            "    0.1          558696          78          7162.8            4347           31422  open64                                                                          \n",
            "    0.0          425076           4        106269.0           29160          240238  sem_wait                                                                        \n",
            "    0.0          261248          89          2935.4            1587            5631  fclose                                                                          \n",
            "    0.0          146914          14         10493.9            1441           15569  write                                                                           \n",
            "    0.0           93508           3         31169.3           24852           41884  fgets                                                                           \n",
            "    0.0           89610          79          1134.3            1005            5739  fcntl                                                                           \n",
            "    0.0           39669          16          2479.3            1236            4577  read                                                                            \n",
            "    0.0           39184          11          3562.2            1989            6012  munmap                                                                          \n",
            "    0.0           35786           5          7157.2            4777           10050  open                                                                            \n",
            "    0.0           16263           3          5421.0            4027            6551  pipe2                                                                           \n",
            "    0.0           14684           2          7342.0            4528           10156  socket                                                                          \n",
            "    0.0           12142           5          2428.4            2016            2639  mprotect                                                                        \n",
            "    0.0           10273           3          3424.3            2082            4785  fread                                                                           \n",
            "    0.0            9401           1          9401.0            9401            9401  connect                                                                         \n",
            "    0.0            8203           2          4101.5            1153            7050  pthread_cond_broadcast                                                          \n",
            "    0.0            2941           1          2941.0            2941            2941  bind                                                                            \n",
            "    0.0            2219           1          2219.0            2219            2219  listen                                                                          \n",
            "\n",
            "\n",
            "\n",
            "\r\n",
            "Generating NVTX Push-Pop Range Statistics...\r\n",
            "NVTX Push-Pop Range Statistics (nanoseconds)\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWfe-ydxm9Jo",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Accelerate and Optimize an N-Body Simulator\n",
        "\n",
        "An [n-body](https://en.wikipedia.org/wiki/N-body_problem) simulator predicts the individual motions of a group of objects interacting with each other gravitationally. [01-nbody.cu](../edit/09-nbody/01-nbody.cu) contains a simple, though working, n-body simulator for bodies moving through 3 dimensional space. The application can be passed a command line argument to affect how many bodies are in the system.\n",
        "\n",
        "In its current CPU-only form, working on 4096 bodies, this application is able to calculate about 30 million interactions between bodies in the system per second. Your task is to:\n",
        "\n",
        "- GPU accelerate the program, retaining the correctness of the simulation\n",
        "- Work iteratively to optimize the simulator so that it calculates over 30 billion interactions per second while working on 4096 bodies `(2<<11)`\n",
        "- Work iteratively to optimize the simulator so that it calculates over 325 billion interactions per second while working on ~65,000 bodies `(2<<15)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pmz42AZPm9Jo",
        "colab_type": "text"
      },
      "source": [
        "### Considerations \n",
        "\n",
        "Here are some things to consider:\n",
        "\n",
        "- Especially for your first refactors, the logic of the application, the `bodyForce` function in particular, can and should remain largely unchanged: focus on accelerating it as easily as possible.\n",
        "- You will not be able to accelerate the `randomizeBodies` function since it is using the `rand` function, which is not available on GPU devices. `randomizeBodies` is a host function. Do not touch it at all.\n",
        "- The codebase contains a for-loop inside `main` for integrating the interbody forces calculated by `bodyForce` into the positions of the bodies in the system. This integration both needs to occur after `bodyForce` runs, and, needs to complete before the next call to `bodyForce`. Keep this in mind when choosing how and where to parallelize."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etdCHA7Gm9Jp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvcc -o nbody 09-nbody/01-nbody.cu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvLRgrVbm9Jr",
        "colab_type": "code",
        "colab": {},
        "outputId": "c0343b9e-d90e-4489-b4a8-50ebf58cfc3f"
      },
      "source": [
        "!./nbody 11 # This argument is passed as `N` in the formula `2<<N`, to determine the number of bodies in the system"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Simulator is calculating positions correctly.\r\n",
            "4096 Bodies: average 34.815 Billion Interactions / second\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMeEJ63rm9Ju",
        "colab_type": "code",
        "colab": {},
        "outputId": "afbf6b37-5529-4b8c-f0b7-361a73972278"
      },
      "source": [
        "!nsys profile --stats=true -o nbody-report ./nbody"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "**** collection configuration ****\n",
            "\toutput_filename = /dli/task/nbody-report\n",
            "\tforce-overwrite = false\n",
            "\tstop-on-exit = true\n",
            "\texport_sqlite = true\n",
            "\tstats = true\n",
            "\tcapture-range = none\n",
            "\tstop-on-range-end = false\n",
            "\tBeta: ftrace events:\n",
            "\tftrace-keep-user-config = false\n",
            "\ttrace-GPU-context-switch = false\n",
            "\tdelay = 0 seconds\n",
            "\tduration = 0 seconds\n",
            "\tkill = signal number 15\n",
            "\tinherit-environment = true\n",
            "\tshow-output = true\n",
            "\ttrace-fork-before-exec = false\n",
            "\tsample_cpu = true\n",
            "\tbacktrace_method = LBR\n",
            "\twait = all\n",
            "\ttrace_cublas = false\n",
            "\ttrace_cuda = true\n",
            "\ttrace_cudnn = false\n",
            "\ttrace_nvtx = true\n",
            "\ttrace_mpi = false\n",
            "\ttrace_openacc = false\n",
            "\ttrace_vulkan = false\n",
            "\ttrace_opengl = true\n",
            "\ttrace_osrt = true\n",
            "\tosrt-threshold = 0 nanoseconds\n",
            "\tcudabacktrace = false\n",
            "\tcudabacktrace-threshold = 0 nanoseconds\n",
            "\tprofile_processes = tree\n",
            "\tapplication command = ./nbody\n",
            "\tapplication arguments = \n",
            "\tapplication working directory = /dli/task\n",
            "\tNVTX profiler range trigger = \n",
            "\tNVTX profiler domain trigger = \n",
            "\tenvironment variables:\n",
            "\tCollecting data...\n",
            "The application process terminated. One or more process it created re-parented. Waiting for termination of re-parented processes. To modify this behavior, use the `--wait` option.\n",
            "\u0000Simulator is calculating positions correctly.\n",
            "4096 Bodies: average 33.117 Billion Interactions / second\n",
            "\tGenerating the /dli/task/nbody-report.qdstrm file.\n",
            "\tCapturing raw events...\n",
            "\t1196 total events collected.\n",
            "\tSaving diagnostics...\n",
            "\tSaving qdstrm file to disk...\n",
            "\tFinished saving file.\n",
            "\n",
            "\n",
            "Importing the qdstrm file using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/QdstrmImporter.\n",
            "\n",
            "Importing...\n",
            "\n",
            "Importing [==================================================100%]\n",
            "Saving report to file \"/dli/task/nbody-report.qdrep\"\n",
            "Report file saved.\n",
            "Please discard the qdstrm file and use the qdrep file instead.\n",
            "\n",
            "Removed /dli/task/nbody-report.qdstrm as it was successfully imported.\n",
            "Please use the qdrep file instead.\n",
            "\n",
            "Exporting the qdrep file to SQLite database using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/nsys-exporter.\n",
            "\n",
            "Exporting 1160 events:\n",
            "\n",
            "0%   10   20   30   40   50   60   70   80   90   100%\n",
            "|----|----|----|----|----|----|----|----|----|----|\n",
            "***************************************************\n",
            "\n",
            "Exported successfully to\n",
            "/dli/task/nbody-report.sqlite\n",
            "\n",
            "Generating CUDA API Statistics...\n",
            "CUDA API Statistics (nanoseconds)\n",
            "\n",
            "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
            "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
            "   97.5       178540169           1     178540169.0       178540169       178540169  cudaMalloc                                                                      \n",
            "    2.4         4435903          20        221795.1           19494          425312  cudaMemcpy                                                                      \n",
            "    0.1          139671          10         13967.1           10758           32579  cudaLaunchKernel                                                                \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Generating CUDA Kernel Statistics...\n",
            "\n",
            "Generating CUDA Memory Operation Statistics...\n",
            "CUDA Kernel Statistics (nanoseconds)\n",
            "\n",
            "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                            \n",
            "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
            "  100.0         3765848          10        376584.8          373650          387540  bodyForce                                                                       \n",
            "\n",
            "\n",
            "CUDA Memory Operation Statistics (nanoseconds)\n",
            "\n",
            "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
            "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
            "   58.7          118863          10         11886.3           11681           12578  [CUDA memcpy HtoD]                                                              \n",
            "   41.3           83629          10          8362.9            8321            8513  [CUDA memcpy DtoH]                                                              \n",
            "\n",
            "\n",
            "CUDA Memory Operation Statistics (KiB)\n",
            "\n",
            "            Total      Operations            Average            Minimum            Maximum  Name                                                                            \n",
            "-----------------  --------------  -----------------  -----------------  -----------------  --------------------------------------------------------------------------------\n",
            "            960.0              10               96.0             96.000               96.0  [CUDA memcpy HtoD]                                                              \n",
            "            960.0              10               96.0             96.000               96.0  [CUDA memcpy DtoH]                                                              \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Generating Operating System Runtime API Statistics...\n",
            "Operating System Runtime API Statistics (nanoseconds)\n",
            "\n",
            "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
            "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
            "   54.4       215305443          12      17942120.2           24684       100119778  sem_timedwait                                                                   \n",
            "   25.3       100045718          11       9095065.3           54495        30568004  poll                                                                            \n",
            "   19.1        75462602         559        134995.7            1013        16048613  ioctl                                                                           \n",
            "    0.7         2634939          78         33781.3            1709          777141  mmap                                                                            \n",
            "    0.2          683353          96          7118.3            1798           17055  fopen                                                                           \n",
            "    0.2          609875           4        152468.7           38304          491671  pthread_create                                                                  \n",
            "    0.1          500994          78          6423.0            4388           10855  open64                                                                          \n",
            "    0.1          259365          89          2914.2            1550            3951  fclose                                                                          \n",
            "    0.0          112699          10         11269.9            7606           14985  write                                                                           \n",
            "    0.0           93480           3         31160.0           23977           42188  fgets                                                                           \n",
            "    0.0           87926          79          1113.0            1023            4749  fcntl                                                                           \n",
            "    0.0           36031           1         36031.0           36031           36031  sem_wait                                                                        \n",
            "    0.0           32527           5          6505.4            4878            8736  open                                                                            \n",
            "    0.0           32200           3         10733.3            4523           21693  pipe2                                                                           \n",
            "    0.0           27108          12          2259.0            1348            3838  read                                                                            \n",
            "    0.0           20162           6          3360.3            2624            4193  munmap                                                                          \n",
            "    0.0           11362           4          2840.5            1756            4553  fread                                                                           \n",
            "    0.0           11048           2          5524.0            4902            6146  socket                                                                          \n",
            "    0.0            9773           4          2443.3            2312            2552  mprotect                                                                        \n",
            "    0.0            7414           1          7414.0            7414            7414  connect                                                                         \n",
            "    0.0            6844           1          6844.0            6844            6844  pthread_cond_broadcast                                                          \n",
            "    0.0            2546           1          2546.0            2546            2546  bind                                                                            \n",
            "    0.0            1936           1          1936.0            1936            1936  listen                                                                          \n",
            "\n",
            "\n",
            "\n",
            "\r\n",
            "Generating NVTX Push-Pop Range Statistics...\r\n",
            "NVTX Push-Pop Range Statistics (nanoseconds)\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oivaGaZym9Jw",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Manual Device Memory Allocation and Copying\n",
        "\n",
        "While `cudaMallocManaged` and `cudaMemPrefetchAsync` are performant, and greatly simplify memory migration, sometimes it can be worth it to use more manual methods for memory allocation. This is particularly true when it is known that data will only be accessed on the device or host, and the cost of migrating data can be reclaimed in exchange for the fact that no automatic on-demand migration is needed.\n",
        "\n",
        "Additionally, using manual device memory management can allow for the use of non-default streams for overlapping data transfers with computational work. In this section you will learn some basic manual device memory allocation and copy techniques, before extending these techniques to overlap data copies with computational work. \n",
        "\n",
        "Here are some CUDA commands for manual device memory management:\n",
        "\n",
        "- `cudaMalloc` will allocate memory directly to the active GPU. This prevents all GPU page faults. In exchange, the pointer it returns is not available for access by host code.\n",
        "- `cudaMallocHost` will allocate memory directly to the CPU. It also \"pins\" the memory, or page locks it, which will allow for asynchronous copying of the memory to and from a GPU. Too much pinned memory can interfere with CPU performance, so use it only with intention. Pinned memory should be freed with `cudaFreeHost`.\n",
        "- `cudaMemcpy` can copy (not transfer) memory, either from host to device or from device to host."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4puCBgwm9Jx",
        "colab_type": "text"
      },
      "source": [
        "### Manual Device Memory Management Example\n",
        "\n",
        "Here is a snippet of code that demonstrates the use of the above CUDA API calls.\n",
        "\n",
        "```cpp\n",
        "int *host_a, *device_a;        // Define host-specific and device-specific arrays.\n",
        "cudaMalloc(&device_a, size);   // `device_a` is immediately available on the GPU.\n",
        "cudaMallocHost(&host_a, size); // `host_a` is immediately available on CPU, and is page-locked, or pinned.\n",
        "\n",
        "initializeOnHost(host_a, N);   // No CPU page faulting since memory is already allocated on the host.\n",
        "\n",
        "// `cudaMemcpy` takes the destination, source, size, and a CUDA-provided variable for the direction of the copy.\n",
        "cudaMemcpy(device_a, host_a, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "kernel<<<blocks, threads, 0, someStream>>>(device_a, N);\n",
        "\n",
        "// `cudaMemcpy` can also copy data from device to host.\n",
        "cudaMemcpy(host_a, device_a, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "verifyOnHost(host_a, N);\n",
        "\n",
        "cudaFree(device_a);\n",
        "cudaFreeHost(host_a);          // Free pinned memory like this.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAP8LNoom9Jx",
        "colab_type": "text"
      },
      "source": [
        "### Manually Allocate Host and Device Memory\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OvcR0uZm9Jx",
        "colab_type": "code",
        "colab": {},
        "outputId": "b60cb3a5-92c8-49f9-987f-e5efae2a26e3"
      },
      "source": [
        "!nvcc -o vector-add-manual-alloc 06-stream-init/solutions/01-stream-init-solution.cu -run"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success! All values calculated correctly.\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpcg_thRm9J0",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Using Streams to Overlap Data Transfers and Code Execution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60z_mJZcm9J2",
        "colab_type": "text"
      },
      "source": [
        "In addition to `cudaMemcpy` is `cudaMemcpyAsync` which can asynchronously copy memory either from host to device or from device to host as long as the host memory is pinned, which can be done by allocating it with `cudaMallocHost`.\n",
        "\n",
        "Similar to kernel execution, `cudaMemcpyAsync` is only asynchronous by default with respect to the host. It executes, by default, in the default stream and therefore is a blocking operation with regard to other CUDA operations occuring on the GPU. The `cudaMemcpyAsync` function, however, takes as an optional 5th argument, a non-default stream. By passing it a non-default stream, the memory transfer can be concurrent to other CUDA operations occuring in other non-default streams.\n",
        "\n",
        "A common and useful pattern is to use a combination of pinned host memory, asynchronous memory copies in non-default streams, and kernel executions in non-default streams, to overlap memory transfers with kernel execution.\n",
        "\n",
        "In the following example, rather than wait for the entire memory copy to complete before beginning work on the kernel, segments of the required data are copied and worked on, with each copy/work segment running in its own non-default stream. Using this technique, work on parts of the data can begin while memory transfers for later segments occur concurrently. Extra care must be taken when using this technique to calculate segment-specific values for the number of operations, and the offset location inside arrays, as shown here:\n",
        "\n",
        "```cpp\n",
        "int N = 2<<24;\n",
        "int size = N * sizeof(int);\n",
        "\n",
        "int *host_array;\n",
        "int *device_array;\n",
        "\n",
        "cudaMallocHost(&host_array, size);               // Pinned host memory allocation.\n",
        "cudaMalloc(&device_array, size);                 // Allocation directly on the active GPU device.\n",
        "\n",
        "initializeData(host_array, N);                   // Assume this application needs to initialize on the host.\n",
        "\n",
        "const int numberOfSegments = 4;                  // This example demonstrates slicing the work into 4 segments.\n",
        "int segmentN = N / numberOfSegments;             // A value for a segment's worth of `N` is needed.\n",
        "size_t segmentSize = size / numberOfSegments;    // A value for a segment's worth of `size` is needed.\n",
        "\n",
        "// For each of the 4 segments...\n",
        "for (int i = 0; i < numberOfSegments; ++i)\n",
        "{\n",
        "  // Calculate the index where this particular segment should operate within the larger arrays.\n",
        "  segmentOffset = i * segmentN;\n",
        "\n",
        "  // Create a stream for this segment's worth of copy and work.\n",
        "  cudaStream_t stream;\n",
        "  cudaStreamCreate(&stream);\n",
        "  \n",
        "  // Asynchronously copy segment's worth of pinned host memory to device over non-default stream.\n",
        "  cudaMemcpyAsync(&device_array[segmentOffset],  // Take care to access correct location in array.\n",
        "                  &host_array[segmentOffset],    // Take care to access correct location in array.\n",
        "                  segmentSize,                   // Only copy a segment's worth of memory.\n",
        "                  cudaMemcpyHostToDevice,\n",
        "                  stream);                       // Provide optional argument for non-default stream.\n",
        "                  \n",
        "  // Execute segment's worth of work over same non-default stream as memory copy.\n",
        "  kernel<<<number_of_blocks, threads_per_block, 0, stream>>>(&device_array[segmentOffset], segmentN);\n",
        "  \n",
        "  // `cudaStreamDestroy` will return immediately (is non-blocking), but will not actually destroy stream until\n",
        "  // all stream operations are complete.\n",
        "  cudaStreamDestroy(stream);\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yp-OT3zzm9J2",
        "colab_type": "text"
      },
      "source": [
        "### Overlap Kernel Execution and Memory Copy Back to Host\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DxsmzN3m9J3",
        "colab_type": "code",
        "colab": {},
        "outputId": "e139e1d5-24c7-4132-a40d-cae4f81d6400"
      },
      "source": [
        "!nvcc -o vector-add-manual-alloc 07-manual-malloc/solutions/01-manual-malloc-solution.cu -run"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success! All values calculated correctly.\r\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}